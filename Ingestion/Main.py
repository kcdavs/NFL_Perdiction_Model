# -*- coding: utf-8 -*-
"""Test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNDexLLqSJa7uz72Qqp9ul_wMLBNtPti

Creating combined table from all the weekly odds data in our github repo
"""

import pandas as pd
import requests
from io import StringIO

# GitHub base URLs
API_BASE = "https://api.github.com/repos/kcdavs/NFL_Perdiction_Model/contents/odds"
RAW_BASE = "https://raw.githubusercontent.com/kcdavs/NFL_Perdiction_Model/main/odds"

all_dfs = []

# Step 1: Get all year folders (2018, 2019, etc.)
years_resp = requests.get(API_BASE)
year_dirs = [item['name'] for item in years_resp.json() if item['type'] == 'dir']

# Step 2: Loop through each year
for year in year_dirs:
    year_url = f"{API_BASE}/{year}"
    weeks_resp = requests.get(year_url)

    # Step 3: Get all weekly CSV files in that year folder
    week_files = [f['name'] for f in weeks_resp.json() if f['name'].endswith(".csv")]

    # Step 4: Loop through each CSV file
    for week_file in week_files:
        raw_url = f"{RAW_BASE}/{year}/{week_file}"

        # Extract numeric week from filename like "week01.csv" or "week1.csv"
        week = int(''.join(filter(str.isdigit, week_file)))

        # Step 5: Download and read the CSV
        csv_text = requests.get(raw_url).text
        df = pd.read_csv(StringIO(csv_text))

        # Add metadata
        df['season'] = int(year)
        df['week'] = week

        all_dfs.append(df)

# Step 6: Combine all weeks + years into one DataFrame
combined_df = pd.concat(all_dfs, ignore_index=True)

"""Flattening above table and joining it to the games table fromthe nflverse github repo"""

import pandas as pd

# 1. Map long team names to short abbreviations (e.g., "Arizona" â†’ "ARI")
team_name_to_abbr = {
    "Arizona": "ARI", "Atlanta": "ATL", "Baltimore": "BAL", "Buffalo": "BUF",
    "Carolina": "CAR", "Chicago": "CHI", "Cincinnati": "CIN", "Cleveland": "CLE",
    "Dallas": "DAL", "Denver": "DEN", "Detroit": "DET", "Green Bay": "GB",
    "Houston": "HOU", "Indianapolis": "IND", "Jacksonville": "JAX", "Kansas City": "KC",
    "Las Vegas": "LV", "LA Chargers": "LAC", "LA Rams": "LAR", "Miami": "MIA",
    "Minnesota": "MIN", "New England": "NE", "New Orleans": "NO", "NY Giants": "NYG",
    "NY Jets": "NYJ", "Philadelphia": "PHI", "Pittsburgh": "PIT", "San Francisco": "SF",
    "Seattle": "SEA", "Tampa Bay": "TB", "Tennessee": "TEN", "Washington": "WAS",
    "N.Y. Giants": "NYG", "N.Y. Jets": "NYJ", "L.A. Rams": "LAR", "L.A. Chargers": "LAC",
    "Oakland": "OAK"
}

# 2. Load odds and games data
odds_url = 'https://raw.githubusercontent.com/kcdavs/NFL_Perdiction_Model/main/odds/2018/week01.csv'
games_url = 'https://raw.githubusercontent.com/nflverse/nfldata/master/data/games.csv'

odds_df = combined_df
games_df = pd.read_csv(games_url)

# 3. Add team abbreviation to odds_df
odds_df["team_abbr"] = odds_df["team"].map(team_name_to_abbr)

# 4. Filter home teams only (rotation % 2 == 1)
flat_odds = odds_df[odds_df['rotation'] % 2 == 0].copy()

# 5. Prepare away team odds by shifting rotation (odd rotation = home, even = away)
away_odds = odds_df.copy()
away_odds['rotation'] += 1  # match with home
away_odds = away_odds[['eid', 'rotation', 'team_abbr', 'opening_ap',
    'ap_8', 'ap_9', 'ap_10', 'ap_16', 'ap_20', 'ap_28', 'ap_29',
    'ap_36', 'ap_44', 'ap_54', 'ap_82', 'ap_84', 'ap_123', 'ap_127', 'ap_130']]

# 6. Merge flat_odds with away_odds on eid and rotation
flat_odds = flat_odds.merge(away_odds, on=['eid', 'rotation'], suffixes=('', '_away'))


# 7. Create a new column for context string (e.g., REG_0_Mon_1910)
games_df['game_context_tag'] = (
    games_df['game_type'] + '_' +
    games_df['div_game'].astype(str) + '_' +
    games_df['weekday'].str[:3] + '_' +
    games_df['gametime'].str[:5].str.replace(':', '', regex=False)
)

# 8. Merge games with odds on season, week, and home team abbreviation
merged = games_df.merge(flat_odds, how='inner',
                        left_on=['season', 'week', 'home_team'],
                        right_on=['season', 'week', 'team_abbr'])

# 9. Select and rename columns in grouped format (adj_X, ap_X, a_ap_X)
odds_sample = merged[[  # keep base metadata first
    'game_id', 'game_context_tag', 'home_score', 'away_score', 'overtime',
    'location', 'roof', 'surface', 'temp', 'wind', 'away_rest', 'home_rest',
    'perc', 'opening_adj',

    # bookmaker 8
    'adj_8', 'ap_8', 'ap_8_away',
    # bookmaker 9
    'adj_9', 'ap_9', 'ap_9_away',
    # bookmaker 10
    'adj_10', 'ap_10', 'ap_10_away',
    # bookmaker 16
    'adj_16', 'ap_16', 'ap_16_away',
    # bookmaker 20
    'adj_20', 'ap_20', 'ap_20_away',
    # bookmaker 28
    'adj_28', 'ap_28', 'ap_28_away',
    # bookmaker 29
    'adj_29', 'ap_29', 'ap_29_away',
    # bookmaker 36
    'adj_36', 'ap_36', 'ap_36_away',
    # bookmaker 44
    'adj_44', 'ap_44', 'ap_44_away',
    # bookmaker 54
    'adj_54', 'ap_54', 'ap_54_away',
    # bookmaker 82
    'adj_82', 'ap_82', 'ap_82_away',
    # bookmaker 84
    'adj_84', 'ap_84', 'ap_84_away',
    # bookmaker 54
    'adj_123', 'ap_123', 'ap_123_away',
    # bookmaker 82
    'adj_127', 'ap_127', 'ap_127_away',
    # bookmaker 84
    'adj_130', 'ap_130', 'ap_130_away'
]]

"""Push code from the above table to our github repo"""

import base64
import requests

# Your GitHub personal access token here
token = "askmeforthetoken"

# Convert your DataFrame to CSV text, then base64 encode it
content = base64.b64encode(odds_sample.to_csv(index=False).encode()).decode()

# GitHub API URL for the exact file path
url = "https://api.github.com/repos/kcdavs/NFL_Perdiction_Model/contents/Model/model_input.csv"

# Prepare JSON payload with commit message and file content
data = {
    "message": "Upload model_input.csv",
    "content": content,
    "branch": "main"
}

# Send PUT request to GitHub API with authorization header
response = requests.put(url, json=data, headers={"Authorization": f"token {token}"})

# Print response status and message
print(response.status_code)
print(response.json())
